{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6823cedd1d31a394",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701d367af866233f",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "zip_file_path_dataset = '/workspace/datasets/isic-2024-challenge.zip'\n",
    "extract_to_dir_dataset = '/workspace/datasets/isic-2024-challenge'\n",
    "os.makedirs(extract_to_dir_dataset, exist_ok=True)\n",
    "with zipfile.ZipFile(zip_file_path_dataset, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_to_dir_dataset)\n",
    "print(f'Files extracted to {extract_to_dir_dataset}')\n",
    "\n",
    "zip_file_path_dataset = '/workspace/datasets/isic-2019-dataset-resized-256.zip'\n",
    "extract_to_dir_dataset = '/workspace/datasets/isic-2019-dataset-resized-256'\n",
    "os.makedirs(extract_to_dir_dataset, exist_ok=True)\n",
    "with zipfile.ZipFile(zip_file_path_dataset, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_to_dir_dataset)\n",
    "print(f'Files extracted to {extract_to_dir_dataset}')\n",
    "\n",
    "zip_file_path_dataset = '/workspace/datasets/isic-2020-dataset-resized-256.zip'\n",
    "extract_to_dir_dataset = '/workspace/datasets/isic-2020-dataset-resized-256'\n",
    "os.makedirs(extract_to_dir_dataset, exist_ok=True)\n",
    "with zipfile.ZipFile(zip_file_path_dataset, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_to_dir_dataset)\n",
    "print(f'Files extracted to {extract_to_dir_dataset}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6759d63760699bd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zip_file_path_dataset = '/workspace/utils.zip'\n",
    "extract_to_dir_dataset = '/workspace/utils'\n",
    "os.makedirs(extract_to_dir_dataset, exist_ok=True)\n",
    "with zipfile.ZipFile(zip_file_path_dataset, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_to_dir_dataset)\n",
    "print(f'Files extracted to {extract_to_dir_dataset}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7188b9e5211bc1c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install pandas matplotlib wandb timm scikit-learn -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998751a7a407a581",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# TRAIN CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1013fab376421e92",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from torchvision.datasets import ImageFolder\n",
    "from utils.data_utils_custom import create_train_valid_folders_custom, get_datasets_based_on_split_option\n",
    "from utils.data_utils_standard import split_dataset_standard, process_standard_datasets\n",
    "from utils.classification_model_utils import train_model, create_model\n",
    "from utils.classification_evaluation import evaluate_model, visualize_dataloader_batch  # Updated import\n",
    "\n",
    "# Configuration dictionary\n",
    "CONFIG = {\n",
    "    'model_name': 'efficientnet_b3.ra2_in1k',\n",
    "    'project_name': 'ISIC_2024_Competition',\n",
    "    'artifact_name': 'isic2024-simplehead-model',\n",
    "    'learning_rate': 0.0003,\n",
    "    'batch_size': 64,\n",
    "    'seed': 42,\n",
    "    'wandb_log': False,\n",
    "\n",
    "    # data options\n",
    "    'dataset_mode': 'combined_custom',  # '2024', '2024prac', '2024_custom', '2024prac_custom', or 'combined', 'combinedprac', 'combined_custom', 'combinedprac_custom'\n",
    "    'split_ratio': 0.2,  # Ratio for validation split\n",
    "    'augmentation': 'default',  # Options: 'default', 'strong', etc.\n",
    "    'sampling': 'default',  # Options: 'default', 'weighted'\n",
    "\n",
    "    # train options\n",
    "    'pretrained': True,\n",
    "    'num_epochs': 20,\n",
    "    'split_option': 'trainprac',  # 'train', 'trainprac', or 'valid'\n",
    "    'head_type': 'simpleheadv2',  # # 'simplehead', 'scsa', 'simpleheadv2'\n",
    "    'valid_model_path': [],\n",
    "    'input_size': (128, 128),  # Change input size here\n",
    "    'loss_function': 'bce',  # 'bce' or 'focal'\n",
    "\n",
    "    # Visualization\n",
    "    'print_backbone': False,\n",
    "    'monte_carlo_visual': False,\n",
    "    'number_of_samples': 5000,\n",
    "\n",
    "    # Sampler\n",
    "    'use_weighted_sampler': False,  # Set to True to use the weighted sampler, False for normal distribution\n",
    "    'scaling_factor': 1,  # Adjust this scaling factor as needed\n",
    "    'freeze': 'unfreeze', # 'freezeall', 'unfreeze', 'freezecustom1'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12eac14a4ac52bf7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# VALID CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eab68c998787dd8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# VALID CONFIG\n",
    "CONFIG = {\n",
    "    'model_name': 'efficientnet_b3.ra2_in1k',\n",
    "    'project_name': 'ISIC_2024_Competition',\n",
    "    'artifact_name': 'isic2024-simplehead-model',\n",
    "    'learning_rate': 0.0003,\n",
    "    'batch_size': 64,\n",
    "    'seed': 42,\n",
    "    'wandb_log': False,\n",
    "    \n",
    "    # data options\n",
    "    'dataset_mode': 'combined_custom',  # '2024', '2024prac', '2024_custom', '2024prac_custom', or 'combined', 'combinedprac', 'combined_custom', 'combinedprac_custom'\n",
    "    'split_ratio': 0.2,  # Ratio for validation split\n",
    "    'augmentation': 'default',  # Options: 'default', 'strong', etc.\n",
    "    'sampling': 'default',  # Options: 'default', 'weighted'\n",
    "\n",
    "    # train options\n",
    "    'pretrained': False,\n",
    "    'num_epochs': 20,\n",
    "    'split_option': 'valid',  # 'train', 'trainprac', or 'valid'\n",
    "    'head_type': 'simpleheadv2',  # # 'simplehead', 'scsa', 'simpleheadv2'\n",
    "    'valid_model_path': [],\n",
    "    'input_size': (128, 128),  # Change input size here\n",
    "    'loss_function': 'bce',  # 'bce' or 'focal'\n",
    "\n",
    "    # Visualization\n",
    "    'print_backbone': False,\n",
    "    'monte_carlo_visual': False,\n",
    "    'number_of_samples': 5000,\n",
    "\n",
    "    # Sampler\n",
    "    'use_weighted_sampler': False,  # Set to True to use the weighted sampler, False for normal distribution\n",
    "    'scaling_factor': 1,  # Adjust this scaling factor as needed\n",
    "    'freeze': 'unfreeze',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3554c408b5b9e60b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# RUN IN ONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1eea395507f66f4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def seeding(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = False\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "    print(\"Seeding done ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_final_class_distribution(train_dir, valid_dir):\n",
    "    train_dataset = ImageFolder(root=train_dir)\n",
    "    valid_dataset = ImageFolder(root=valid_dir)\n",
    "\n",
    "    train_labels = [sample[1] for sample in train_dataset.samples]\n",
    "    valid_labels = [sample[1] for sample in valid_dataset.samples]\n",
    "\n",
    "    train_distribution = Counter(train_labels)\n",
    "    valid_distribution = Counter(valid_labels)\n",
    "\n",
    "    print(f\"Final training dataset class distribution: {dict(train_distribution)}\")\n",
    "    print(f\"Final validation dataset class distribution: {dict(valid_distribution)}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    seeding(CONFIG['seed'])\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Define model save prefix\n",
    "    model_save_prefix = f\"{CONFIG['model_name'].replace('.', '')}_{CONFIG['head_type']}_{CONFIG['dataset_mode']}_{CONFIG['augmentation']}_{CONFIG['sampling']}_loss{CONFIG['loss_function']}_imgsz{CONFIG['input_size'][0]}\"\n",
    "\n",
    "    experiment_dir = os.path.join(\"./logs\", model_save_prefix)\n",
    "    plots_dir = os.path.join(experiment_dir, \"plots\")\n",
    "    os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "    if CONFIG['dataset_mode'] in ['2024_custom', '2024prac_custom', 'combined_custom', 'combinedprac_custom']:\n",
    "        train_dir, valid_dir = create_train_valid_folders_custom(CONFIG)\n",
    "        train_loader, valid_loader = get_datasets_based_on_split_option(train_dir, valid_dir, CONFIG)\n",
    "    else:\n",
    "        train_df, valid_df = split_dataset_standard(CONFIG)\n",
    "        train_loader, valid_loader = process_standard_datasets(CONFIG)\n",
    "\n",
    "    if CONFIG['split_option'] == 'trainprac':\n",
    "        visualize_dataloader_batch(train_loader, \"Training Practice Data Batch\")\n",
    "        model = train_model(train_loader, valid_loader, CONFIG, device, model_save_prefix)\n",
    "\n",
    "    elif CONFIG['split_option'] == 'valid':\n",
    "        visualize_dataloader_batch(valid_loader, \"Validation Data Batch\")\n",
    "        for model_path in CONFIG['valid_model_path']:\n",
    "            model = create_model(CONFIG, device, pretrained=False)  # Do not load pretrained weights for evaluation\n",
    "            model.load_state_dict(torch.load(model_path))\n",
    "            model = model.to(device)\n",
    "            print(f\"Evaluating model: {model_path}\")\n",
    "\n",
    "            # Evaluate the model for all epochs (assuming single evaluation call, no epoch multiple condition)\n",
    "            evaluate_model(model, valid_loader, device, epoch=None, save_dir=plots_dir)  # Save evaluation results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78201c7849ccabbd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10928ce367832b66",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Prepare the Results for download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9104f2d277cd594",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f319cfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_folder_and_copy_notebook(folder_path, notebook_path, suffix):\n",
    "    # Determine the parent directory of the folder and notebook\n",
    "    folder_parent_dir = os.path.dirname(folder_path)\n",
    "    notebook_parent_dir = os.path.dirname(notebook_path)\n",
    "    \n",
    "    # Extract the original names of the folder and notebook\n",
    "    folder_name = os.path.basename(folder_path)\n",
    "    notebook_name, notebook_ext = os.path.splitext(os.path.basename(notebook_path))\n",
    "    \n",
    "    # Create new names with the given suffix\n",
    "    new_folder_name = f\"{folder_name}_{suffix}\"\n",
    "    new_notebook_name = f\"{notebook_name}_{suffix}{notebook_ext}\"\n",
    "    \n",
    "    # Create new paths for the folder and notebook\n",
    "    new_folder_path = os.path.join(folder_parent_dir, new_folder_name)\n",
    "    new_notebook_path = os.path.join(notebook_parent_dir, new_notebook_name)\n",
    "    \n",
    "    # Rename the folder by moving it to the new path with the new name\n",
    "    shutil.move(folder_path, new_folder_path)\n",
    "    \n",
    "    # Copy the notebook file with the new name\n",
    "    shutil.copy2(notebook_path, new_notebook_path)\n",
    "    \n",
    "    # Set the output zip file name based on the new folder name\n",
    "    output_zip_file = os.path.join(folder_parent_dir, f\"{new_folder_name}.zip\")\n",
    "    \n",
    "    # Compress the renamed folder into a ZIP file\n",
    "    with zipfile.ZipFile(output_zip_file, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for root, dirs, files in os.walk(new_folder_path):\n",
    "            for file in files:\n",
    "                # Create the full path of the file\n",
    "                full_path = os.path.join(root, file)\n",
    "                # Add file to the zip file with its relative path\n",
    "                relative_path = os.path.relpath(full_path, os.path.join(new_folder_path, '..'))\n",
    "                zipf.write(full_path, relative_path)\n",
    "    \n",
    "    # Optionally, remove the renamed folder after compression to clean up\n",
    "    shutil.rmtree(new_folder_path)\n",
    "    \n",
    "    print(f'Folder {folder_path} renamed to {new_folder_name} and compressed into {output_zip_file}')\n",
    "    print(f'Notebook {notebook_path} copied to {new_notebook_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329fcd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "folder_to_compress = '/workspace/logs'\n",
    "notebook_to_copy = '/workspace/train_vastai.ipynb'\n",
    "suffix = 'simplev2_classification'\n",
    "compress_folder_and_copy_notebook(folder_to_compress, notebook_to_copy, suffix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
