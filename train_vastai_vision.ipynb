{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6823cedd1d31a394",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701d367af866233f",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "zip_file_path_dataset = '/workspace/datasets/isic-2024-challenge.zip'\n",
    "extract_to_dir_dataset = '/workspace/datasets/isic-2024-challenge'\n",
    "os.makedirs(extract_to_dir_dataset, exist_ok=True)\n",
    "with zipfile.ZipFile(zip_file_path_dataset, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_to_dir_dataset)\n",
    "print(f'Files extracted to {extract_to_dir_dataset}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7188b9e5211bc1c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": "!pip install pandas wandb timm scikit-learn"
  },
  {
   "cell_type": "markdown",
   "id": "998751a7a407a581",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# TRAIN CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1013fab376421e92",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timm\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import BCELoss\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "\n",
    "# Configuration dictionary\n",
    "config = {\n",
    "    'batch_size': 64,\n",
    "    'num_classes': 1,  # Binary classification (single output with logits)\n",
    "    'learning_rate': 1e-4,\n",
    "    'num_epochs': 35,\n",
    "    'seed': 42,\n",
    "    'model_names': [  # List of models to train\n",
    "        'selecsls42b.in1k',\n",
    "        # 'nextvit_small.bd_in1k_384',\n",
    "        # 'efficientnet_b3.ra2_in1k'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Seeding function\n",
    "def seeding(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = False\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "    print(\"Seeding done ...\")\n",
    "\n",
    "# Dataset Preprocessing (2024 Only)\n",
    "def load_and_preprocess_data():\n",
    "    # Load 2024 metadata\n",
    "    isic2024_path = './datasets/isic-2024-challenge/train-metadata.csv'\n",
    "    df_2024 = pd.read_csv(isic2024_path)\n",
    "\n",
    "    # Add a column for the image path based on the 'isic_id'\n",
    "    df_2024['image_path'] = './datasets/isic-2024-challenge/train-image/image/' + df_2024['isic_id'] + '.jpg'\n",
    "\n",
    "    # Add a column to indicate the year (for reference or potential further use)\n",
    "    df_2024['year'] = 2024\n",
    "\n",
    "    # Return the 2024 dataset\n",
    "    return df_2024\n",
    "\n",
    "# Custom Dataset Class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.dataframe.iloc[idx]['image_path']\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = self.dataframe.iloc[idx]['target']\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# DataLoader Function with Model-Specific Transforms\n",
    "def get_dataloader(config, dataframe, model_name):\n",
    "    # Resolve model-specific data config\n",
    "    base_model = timm.create_model(model_name, pretrained=True, num_classes=config['num_classes'])\n",
    "    data_config = timm.data.resolve_model_data_config(base_model)\n",
    "    transform = timm.data.create_transform(**data_config, is_training=False)\n",
    "\n",
    "    dataset = CustomDataset(dataframe=dataframe, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "# Training Loop\n",
    "def train_model(model_name, config, train_dataloader):\n",
    "    print(f\"Training model: {model_name}\")\n",
    "\n",
    "    # Create a directory to save model weights for each epoch\n",
    "    model_dir = os.path.join('./logs', model_name)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    # Load the model\n",
    "    base_model = timm.create_model(model_name, pretrained=True, num_classes=config['num_classes'])  # Binary classification\n",
    "\n",
    "    # Add sigmoid layer\n",
    "    model = nn.Sequential(\n",
    "        base_model,\n",
    "        nn.Sigmoid()  # Adds a sigmoid layer to the model\n",
    "    )\n",
    "\n",
    "    # Move the model to the appropriate device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = BCELoss()  # BCELoss expects probabilities (after sigmoid)\n",
    "    optimizer = AdamW(model.parameters(), lr=config['learning_rate'])\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(config['num_epochs']):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{config['num_epochs']}\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device).float().unsqueeze(\n",
    "                1)  # Convert labels to float and reshape\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Print loss for this epoch\n",
    "        avg_loss = running_loss / len(train_dataloader)\n",
    "        print(f\"Epoch [{epoch + 1}/{config['num_epochs']}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Save model weights after every epoch\n",
    "        model_path = os.path.join(model_dir, f'epoch_{epoch + 1}.pth')\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"Saved model weights to {model_path}\")\n",
    "\n",
    "    print(f\"Finished training model: {model_name}\\n\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# Main Function to Combine Everything\n",
    "def main():\n",
    "    # Set the seed\n",
    "    seeding(config['seed'])\n",
    "\n",
    "    # Load and preprocess the data\n",
    "    df_train = load_and_preprocess_data()\n",
    "\n",
    "    # Train each model\n",
    "    for model_name in config['model_names']:\n",
    "        # Create DataLoader with model-specific transforms\n",
    "        train_dataloader = get_dataloader(config, df_train, model_name)\n",
    "        model = train_model(model_name, config, train_dataloader)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Results Saving",
   "id": "4b4abe03d3843cbf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f319cfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def compress_folder_and_copy_notebook(folder_path, notebook_path, suffix):\n",
    "    # Determine the parent directory of the folder and notebook\n",
    "    folder_parent_dir = os.path.dirname(folder_path)\n",
    "    notebook_parent_dir = os.path.dirname(notebook_path)\n",
    "    \n",
    "    # Extract the original names of the folder and notebook\n",
    "    folder_name = os.path.basename(folder_path)\n",
    "    notebook_name, notebook_ext = os.path.splitext(os.path.basename(notebook_path))\n",
    "    \n",
    "    # Create new names with the given suffix\n",
    "    new_folder_name = f\"{folder_name}_{suffix}\"\n",
    "    new_notebook_name = f\"{notebook_name}_{suffix}{notebook_ext}\"\n",
    "    \n",
    "    # Create new paths for the folder and notebook\n",
    "    new_folder_path = os.path.join(folder_parent_dir, new_folder_name)\n",
    "    new_notebook_path = os.path.join(notebook_parent_dir, new_notebook_name)\n",
    "    \n",
    "    # Rename the folder by moving it to the new path with the new name\n",
    "    shutil.move(folder_path, new_folder_path)\n",
    "    \n",
    "    # Copy the notebook file with the new name\n",
    "    shutil.copy2(notebook_path, new_notebook_path)\n",
    "    \n",
    "    # Set the output zip file name based on the new folder name\n",
    "    output_zip_file = os.path.join(folder_parent_dir, f\"{new_folder_name}.zip\")\n",
    "    \n",
    "    # Compress the renamed folder into a ZIP file\n",
    "    with zipfile.ZipFile(output_zip_file, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for root, dirs, files in os.walk(new_folder_path):\n",
    "            for file in files:\n",
    "                # Create the full path of the file\n",
    "                full_path = os.path.join(root, file)\n",
    "                # Add file to the zip file with its relative path\n",
    "                relative_path = os.path.relpath(full_path, os.path.join(new_folder_path, '..'))\n",
    "                zipf.write(full_path, relative_path)\n",
    "    \n",
    "    # Optionally, remove the renamed folder after compression to clean up\n",
    "    shutil.rmtree(new_folder_path)\n",
    "    \n",
    "    print(f'Folder {folder_path} renamed to {new_folder_name} and compressed into {output_zip_file}')\n",
    "    print(f'Notebook {notebook_path} copied to {new_notebook_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329fcd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "folder_to_compress = '/workspace/logs'\n",
    "notebook_to_copy = '/workspace/train_vastai.ipynb'\n",
    "suffix = 'effnet3'\n",
    "compress_folder_and_copy_notebook(folder_to_compress, notebook_to_copy, suffix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
